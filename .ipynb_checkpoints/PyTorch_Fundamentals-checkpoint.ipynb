{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b8a5a7-2332-446b-b7b1-0ca8d133c709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA version: 11.8\n",
      "GPU available: True\n",
      "GPU Name: NVIDIA GeForce GTX 1650 SUPER\n",
      "GPU capabilities:  ['sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_37', 'compute_37']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU capabilities: \", torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19f70efa-ff72-4ee7-87eb-378194d7673f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#Introduction to tensors\n",
    "#scalars\n",
    "scalar = torch.tensor(7)\n",
    "scalar\n",
    "scalar.item()\n",
    "\n",
    "\n",
    "#vectors\n",
    "vector = torch.tensor([7,7])\n",
    "vector\n",
    "vector.ndim\n",
    "vector.shape\n",
    "\n",
    "#matrix\n",
    "matrix = torch.tensor([[7,8],\n",
    "                       [9,10]])\n",
    "matrix\n",
    "matrix.ndim\n",
    "matrix.shape\n",
    "\n",
    "#tensor\n",
    "tensor = torch.tensor([[[1,2,3],\n",
    "                        [3,6,9],\n",
    "                        [2,4,5]]])\n",
    "tensor\n",
    "tensor.ndim\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f45257d7-e777-4ff7-bcea-d5a1eaae084c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Random tensors\n",
    "random_tensor = torch.rand(3,4)\n",
    "random_tensor\n",
    "\n",
    "random_tensor.ndim\n",
    "\n",
    "# create random tensor with similar shape to an image tensor\n",
    "random_image_size_tensor = torch.rand(size=(3,224,224))\n",
    "random_image_size_tensor.shape, random_image_size_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ad3b82a5-39f8-4e28-a174-42dc41c41131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of tensor: torch.int64\n",
      "Shape of tensor: torch.Size([2])\n",
      "Device of tensor: cpu\n",
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n",
      "tensor(14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 27,  30,  33],\n",
       "        [ 61,  68,  75],\n",
       "        [ 95, 106, 117]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1. Zeros and ones\n",
    "zeros = torch.zeros(size=(3,4))\n",
    "zeros\n",
    "\n",
    "ones = torch.ones(size=(3,4))\n",
    "ones\n",
    "\n",
    "ones.dtype\n",
    "\n",
    "\n",
    "## Create a range of tensors\n",
    "one_to_ten = torch.arange(1,11)\n",
    "one_to_ten\n",
    "\n",
    "one_to_ten_two_steps = torch.arange(start=1,end=11,step=2)\n",
    "one_to_ten_two_steps\n",
    "\n",
    "## creating tensors like\n",
    "ten_zeros = torch.zeros_like(input=one_to_ten)\n",
    "ten_zeros\n",
    "\n",
    "## 2. Tensor datatypes\n",
    "# *note*: Tensor datatypes is one of the 3 big error sources you'll run into with PyTorch & deep learning:\n",
    "# i. Tensors not right datatype\n",
    "# ii. Tensors not right shape\n",
    "# iii. Tensors not on the right device\n",
    "float32_tensor = torch.tensor([3.0,6.0,9.0],\n",
    "                              dtype=None, # what datatype is the tensor (torch.tensor has data types)\n",
    "                              device=None,\n",
    "                              requires_grad=False)\n",
    "float32_tensor\n",
    "\n",
    "#float16_tensor = float32_tensor.type(torch.float16)\n",
    "int32_tensor = torch.tensor([3,6,9],\n",
    "                           dtype=torch.int32)\n",
    "\n",
    "int32_tensor * float32_tensor\n",
    "\n",
    "\n",
    "## 3. Tensor Attributes\n",
    "#. datatype - tensor.dtype\n",
    "#. shape - tensor.shape\n",
    "#. device - tensor.device\n",
    "\n",
    "some_tensor = torch.tensor([3,4])\n",
    "some_tensor\n",
    "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
    "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
    "print(f\"Device of tensor: {some_tensor.device}\")\n",
    "\n",
    "\n",
    "## 4. Manipulating Tensors (Tensor operations)\n",
    "# Tensor operations include: addition, subtraction etc.\n",
    "\n",
    "t1 = torch.tensor([1,2,3])\n",
    "t1 + 10\n",
    "t1 * 10\n",
    "t1 - 10\n",
    "\n",
    "# PyTorch in-built functions: .mul, .add, etc\n",
    "#torch.mul(t1,10)\n",
    "\n",
    "# Element-wise & Matrix multiplication\n",
    "print(t1, '*', t1)\n",
    "print(f\"Equals: {t1 * t1}\")\n",
    "\n",
    "print(torch.matmul(t1,t1)) # equivalent to t1 @ t1\n",
    "\n",
    "## Matrix multiplication rules:\n",
    "# i. inner dimensions must match e.g: (3,2) @ (2,3) is fine as inner-dims are of size 2, but (3, 2) @ (3, 2) won't work\n",
    "# ii. resulting matrix has shape of outer dimensions. i.e. (2, 3) @ (3, 2) produces a (2, 2) sized matrix\n",
    "\n",
    "## Shapes for matrix multiplication\n",
    "tA = torch.tensor([[1,2],\n",
    "                   [3,4],\n",
    "                   [5,6]])\n",
    "tB = torch.tensor([[7,10],\n",
    "                   [8,11],\n",
    "                   [9,12]])\n",
    "tA.shape, tB.shape\n",
    "\n",
    "# We can manipulate a tensor using a **transpose**\n",
    "tB = tB.T\n",
    "torch.mm(tA, tB) # torch.mm is an alias for torch.matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c726daf-ee06-4584-834a-d60f32d10c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "670893e1-61fd-439a-8d5a-7ff21b975be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 2])\n",
      "New shape: torch.Size([2, 224, 224])\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([9])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tensor aggregation: finding the min,max,mean,sum of tensor values\n",
    "x = torch.arange(0,100,10)\n",
    "x\n",
    "torch.min(x), x.min()\n",
    "torch.max(x), x.max()\n",
    "torch.mean(x.type(torch.float32)), x.type(torch.float32).mean() # torch.mean requires a float32 datatype to work\n",
    "\n",
    "torch.sum(x), x.sum()\n",
    "\n",
    "# Get minimum value index position: argmin(), argmax()\n",
    "x.argmax()\n",
    "\n",
    "## Reshaping, stacking, squeezing and unsqueezing tensors\n",
    "# Reshaping - reshapes an input tensor to a defined shape\n",
    "# View - return a view of an input tensor of certain shape but keep the same memory\n",
    "# Stacking - combine multiple tensors on top of each other\n",
    "# Squeeze - removes all '1' dimensions from a tensor\n",
    "# Unsqueeze - add a '1' dimension to a target tensor\n",
    "# Permute = return a view of the input with dimensions permuted(swapped) in a certain way\n",
    "y = torch.arange(1.,10.)\n",
    "y, y.shape\n",
    "\n",
    "# Add an extra dimension\n",
    "y_reshaped = y.reshape(1,9)\n",
    "y_reshaped, y_reshaped.shape\n",
    "\n",
    "# change the view - changing the view, z below, also changes the source, x, bc a view of a tensor shares the same memory as the original tensor\n",
    "z = y.view(1,9)\n",
    "z[:,0] = 5\n",
    "y\n",
    "z, z.shape \n",
    "\n",
    "# stack tensors on top of each other (default is with a dim = 1), there's also vstack and hstack to look into\n",
    "y_stacked = torch.stack([x,x,x])\n",
    "y_stacked\n",
    "\n",
    "# Squeezing & Unsqueezing\n",
    "# torch.squeeze removes all single dimensions. for y_reshaped, it changes it's shape from 1,9 -> 9\n",
    "y_reshaped.squeeze()\n",
    "\n",
    "# torch.permute - rearranges dimensions as desired e.g torch.permute(x,(2,0,1)) moves 2nd dim to 0, 0 -> 1 and 1 -> 2\n",
    "# p.s. permute returns a view i.e. it refers to the same original data location\n",
    "x_og = torch.rand(size=(224,224,2)) # [height, weight, color_channels]\n",
    "\n",
    "# permute original, x_og, tensor to rearrange the axis (or dim) order\n",
    "x_permuted = x_og.permute(2,0,1)\n",
    "print(f\"Previous shape: {x_og.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")\n",
    "\n",
    "## (Indexing) Select data from tensors\n",
    "r = torch.arange(1,10).reshape(1,3,3)\n",
    "r, r.shape\n",
    "\n",
    "# the semicolon, :, is used to select \"ALL\" for a target dimension\n",
    "print(r)\n",
    "r[:, 2]\n",
    "#r[:, :, 2]\n",
    "#r[0, 0, :]\n",
    "r[:,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dcb6f773-a60b-46dd-81dc-878b51438096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch tensors & NumPy (python numerical computing library)\n",
    "# demonstrate numpy array to tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "arr = np.arange(1.0,8.0)\n",
    "tensr = torch.from_numpy(arr) # Note: when converting numpy -> pytorch, pytorch reflects numpy's default datatype of float64 unless otherwise specified\n",
    "arr, tensr\n",
    "\n",
    "# change the value of array (does NOT update tensor and vice-versa. i.e. they do NOT share memory)\n",
    "arr = arr + 1\n",
    "arr, tensr\n",
    "\n",
    "# Go from tensor to numpy\n",
    "tensr2 = torch.ones(7)\n",
    "numpy_tensr = tensor.numpy()\n",
    "tensr2, numpy_tensr\n",
    "\n",
    "#numpy_tensr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "527adbf1-a6a4-42f7-a8f3-415763ab27f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "# in short, how a neural network learns:\n",
    "# start with random numbers -> tensor operations -> update random nums to try and make them better representations of the data -> repeat -> repeat...\n",
    "\n",
    "# to reduce the randomness in neural networks, we use the random seed\n",
    "import torch\n",
    "rand_seed = 42\n",
    "\n",
    "torch.manual_seed(rand_seed)\n",
    "rand_tens_A = torch.rand(3,4)\n",
    "\n",
    "torch.manual_seed(rand_seed)\n",
    "rand_tens_B = torch.rand(3,4)\n",
    "\n",
    "print(rand_tens_A) \n",
    "print(rand_tens_B)\n",
    "\n",
    "print(rand_tens_A == rand_tens_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "518a34f7-f242-4c3b-97aa-4504643f7ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Running tensors/pytorch objects on GPUs and making faster computations\n",
    "#!nvidia-smi\n",
    "import torch\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device\n",
    "\n",
    "torch.cuda.device_count()\n",
    "\n",
    "# Putting tensors (and models) on the GPU\n",
    "tensor = torch.tensor([1,2,3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "#print(tensor, tensor.device)\n",
    "\n",
    "# Move Tensor to GPU if available\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu\n",
    "\n",
    "# Moving tensor back to CPU (to use things like numpy, which cannot be used on GPU)\n",
    "tensor_back_to_cpu = tensor_on_gpu.cpu().numpy() # or tensor_on_gpu.to('cpu')\n",
    "tensor_back_to_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05408199-42e7-4b48-9139-fb5f3367e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises\n",
    "#00 learnpytorch.io/00_pytorch_fundamentals\n",
    "# do fundamentals exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df4b8d-e6ba-4a2b-bc68-653c42ccdce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
